# ViSpec: Vision-Aware Speculative Decoding

## Bibliographic Information

**Title:** ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding

**Authors:** J. Kang, et al.

**arXiv:** [2509.15235](https://arxiv.org/abs/2509.15235)

**Published:** September 2025

**Venue:** NeurIPS 2025 (accepted)

**Citations:** 2+ (as of January 2026)

**PDF:** [OpenReview](https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf)

**GitHub:** [KangJialiang/ViSpec](https://github.com/KangJialiang/ViSpec)

---

## Abstract Summary

ViSpec introduces **Vision-Aware Speculative Decoding**, a method to accelerate Vision-Language Models by addressing the unique challenge of high-redundancy visual information. The key innovation is a lightweight vision adapter that enables efficient drafting for VLMs.

---

## Key Contributions

### 1. Vision Adapter

**Problem:** Standard speculative decoding assumes draft model quality affects acceleration speed. For VLMs, this creates issues because:
- Vision features have high redundancy
- Direct prediction is expensive
- Draft models struggle with visual inputs

**Solution:** Vision adapter module
```python
class VisionAdapter(nn.Module):
    """
    Lightweight adapter for vision features in speculative decoding
    """
    def __init__(self, vision_dim, hidden_dim):
        self.adapter = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, vision_dim)  # Reconstruction
        )

    def forward(self, vision_features):
        # Compress and decompress vision features
        compressed = self.compress(vision_features)
        reconstructed = self.decompress(compressed)
        return reconstructed
```

**Key insight:** Adapter allows smaller draft model to handle vision features efficiently

### 2. Performance Results

**Main Result:** Up to **3.22× speedup** for mainstream VLMs

**Benchmarks:**
| Model | Task | Baseline (tokens/s) | ViSpec (tokens/s) | Speedup |
|-------|------|-------------------|------------------|--------|
| LLaVA-1.5 | VQA | 18.5 | 59.6 | 3.22x |
| LLaVA-1.5 | Captioning | 22.1 | 65.3 | 2.95x |
| LLaVA-1.5 | OCR | 35.8 | 108.7 | 3.04x |

**Quality Preservation:** No degradation in task performance

---

## Technical Approach

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    ViSpec Pipeline                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Vision Input ──► Vision Encoder (Target)                       │
│                    │                                     │
│                    ▼                                     │
│              Vision Features (cached)                             │
│                    │                                     │
│   ┌────────────────────────────────────────────────┐      │
│   │  Vision Adapter (lightweight)                    │      │
│   │  - Compress high-redundancy features             │      │
│   │  - Enable smaller draft model                   │      │
│   └────────────────────────────────────────────────┘      │
│                    │                                     │
│                    ▼                                     │
│              Draft Model (small)                            │
│                    │                                     │
│                    ▼                                     │
│         Draft Tokens (k candidates)                          │
│                    │                                     │
│   ┌────────────────────────────────────────────────┐      │
│   │  Target VLM Verification (parallel)                │      │
│   │  - Accept high-quality candidates               │      │
│   │  - Reject low-probability candidates            │      │
│   └────────────────────────────────────────────────┘      │
│                    │                                     │
│                    ▼                                     │
│              Final Output (quality preserved)               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Algorithm

### Draft Generation

```python
def generate_draft_with_vision_adapter(vision_features, text_context, draft_model):
    """
    Generate draft tokens using vision-adapted features
    """
    # Compress vision features
    compressed_features = vision_adapter.compress(vision_features)

    # Expand back to dimension
    adapted_features = vision_adapter.expand(compressed_features)

    # Generate draft tokens
    draft_tokens = []
    current_context = text_context

    for _ in range(max_draft_tokens):
        # Combine adapted vision features with text
        combined = combine(adapted_features, current_context)

        # Predict next token
        next_token = draft_model(combined)
        draft_tokens.append(next_token)

        # Update context
        current_context = torch.cat([current_context, next_token], dim=-1)

    return draft_tokens
```

### Verification

Standard EAGLE-2 style verification with tree attention:
```python
def verify_with_target_vlm(draft_tokens, target_vlm):
    """
    Verify draft tokens using target VLM
    """
    # Create sequence for verification
    full_sequence = torch.cat([
        context,
        draft_tokens
    ], dim=-1)

    # Get target VLM probabilities
    target_probs = target_vlm(full_sequence)

    # Accept or reject each token
    accepted = []
    for i, token_id in enumerate(draft_tokens):
        # Acceptance probability
        p_target = target_probs[i, token_id]
        p_draft = draft_probs[i, token_id]

        alpha = min(1.0, p_target / p_draft)

        if random() < alpha:
            accepted.append(token_id)
        else:
            # Resample
            p_adjusted = clamp(p_target - p_draft, min=0)
            resampled = sample(p_adjusted)
            accepted.append(resampled)
            break  # Stop after rejection

    return accepted
```

---

## Key Innovation: Draft Model Performance vs. Speed

### Critical Finding

**Traditional wisdom:** Better draft model → faster acceleration

**ViSpec Discovery:** For VLMs, draft model quality does NOT directly determine speed. Instead:
- **Vision adapter quality** matters more
- **Feature alignment** between draft and target is critical
- **Redundancy reduction** in vision features is key

### Mathematical Insight

```
Speedup = f(adapter_quality, alignment, redundancy_removal)

NOT Speedup = f(draft_model_quality)
```

This explains why small models with good adapters outperform large models without.

---

## Experimental Analysis

### Ablation Study

| Component | Speedup without | Speedup with | Improvement |
|-----------|------------------|---------------|-------------|
| Vision adapter | 2.1x | 3.22x | +53% |
| Tree verification | 2.8x | 3.22x | +15% |
| Multi-layer features | 2.9x | 3.22x | +11% |
| Training with adapters | 2.3x | 3.22x | +40% |

### Vision Adapter Architecture Comparison

| Architecture | Params | Speedup | Quality Impact |
|--------------|--------|--------|---------------|
| Linear projection | 1M | 2.8x | None |
| 2-layer MLP | 2M | 3.22x | None |
| 3-layer MLP | 3M | 3.18x | None |
| Transformer adapter | 5M | 3.15x | None |

**Result:** 2-layer MLP is optimal

---

## Relevance to EventGPT → VideoLLaVA

### Direct Applications

✅ **High relevance** for video understanding:

1. **Event Camera Processing**
   - ViSpec's vision adapter could help align event features
   - Events are sparse, similar to ViSpec's high-redundancy case
   - EventGPT features could serve as input to adapter

2. **Feature Alignment**
   - Adapter layer could map EventGPT → VideoLLaVA feature space
   - Similar to ViSpec's vision adapter concept
   - Training with KL divergence

3. **Draft Model Selection**
   - Our 1B EventGPT LM could serve as draft model
   - ViSpec shows small models work well with good adapters

### Implementation Approach

```python
# EventGPT → VideoLLaVA with ViSpec-inspired adapter

class EventGPTToVideoLLaVAAdapter:
    """
    Adapter inspired by ViSpec for EventGPT → VideoLLaVA alignment
    """
    def __init__(self, eventgpt_dim, videollava_dim, hidden_dim):
        # Compress EventGPT features
        self.compress = nn.Linear(eventgpt_dim, hidden_dim)
        # Expand to VideoLLaVA dimension
        self.expand = nn.Linear(hidden_dim, videollava_dim)

    def forward(self, eventgpt_features):
        """
        Align EventGPT sparse event features to VideoLLaVA dense features
        """
        compressed = self.compress(eventgpt_features)
        aligned = self.expand(compressed)
        return aligned

# Usage in speculative decoding
aligned_features = adapter(eventgpt_features)
draft_tokens = eventgpt_lm(aligned_features, text_context)
verified = videollava.verify(draft_tokens)
```

### Expected Benefits

| Aspect | Without Adapter | With ViSpec-style Adapter |
|--------|-------------------|------------------------|
| Acceptance rate | 40-50% | 70-80% |
| Speedup | 1.5-2x | 2.5-3x |
| Training time | 2-3 days | 4-5 days |
| Memory overhead | +2GB | +3GB (adapter) |

---

## Comparison with Related Works

| Method | Year | Speedup | Focus |
|--------|------|--------|-------|
| EAGLE (LLM) | 2024 | 3-4x | Feature-level for LLMs |
| SpecVLM | 2025 | 2.5-2.9x | EAGLE-2 for VLMs |
| **ViSpec** | 2025 | 3.22x | Vision-aware adapter |
| **Our Approach** | 2026 | TBD | Sparse events + adapter |

---

## Strengths

1. **Groundbreaking insight:** Draft model quality ≠ speed
2. **Practical solution:** Vision adapter is simple and effective
3. **Strong results:** Consistent 3+× speedup across tasks
4. **Code availability:** GitHub repository provided

---

## Limitations

1. **Vision adapter must be trained** - not automatic
2. **Task-specific optimization** - adapter tuned per task
3. **Still limited by vision encoding cost** - only generation accelerated
4. **Adapter overhead** - adds inference latency

---

## Future Directions

### Open Questions

1. Can vision adapter be shared across different VLMs?
2. How to accelerate vision encoding itself?
3. Optimal adapter architecture for different vision tasks?
4. Can we learn the adapter automatically?

### Research Opportunities

1. **Unified adapter** for multi-VLM scenarios
2. **Dynamic adapter** selection based on input
3. **Adapter compression** for deployment
4. **Joint training** of adapter and draft model

---

## Citation

```bibtex
@inproceedings{kang2025vispec,
  title={ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding},
  author={Kang, Jialiang and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2025},
  url={https://openreview.net/forum?id=x2BsIdJJJW}
}
```

---

## Summary

**ViSpec** provides a crucial insight for our EventGPT → VideoLLaVA work:

> "For VLMs, draft model quality does NOT directly determine speed. Instead, vision adapter quality and feature alignment matter more."

This is valuable because:
1. Our EventGPT encoder is small but has unique event features
2. A well-trained adapter could bridge EventGPT → VideoLLaVA gap
3. The adapter approach is simpler than full model alignment

**Recommendation:** Implement a ViSpec-style adapter as part of our training pipeline.

---

**Paper Rating:** ⭐⭐⭐⭐⭐ (Essential)

**Last Updated:** January 23, 2026
**Status:** ✅ Complete
