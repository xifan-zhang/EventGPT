{
  "task": "train_L4_Attention",
  "vlm_only": false,
  "adapter": {
    "level": 4,
    "name": "Attention",
    "hidden_dim": 4096,
    "bottleneck_dim": 256,
    "num_blocks": 3,
    "num_heads": 8,
    "num_layers": 1,
    "ffn_dim": 2048,
    "params": 100714497
  },
  "data": {
    "train_path": "./feasible/feature_alignment/data/chunked_train_1s_4bit",
    "val_path": "./feasible/feature_alignment/data/chunked_test_1s_4bit",
    "train_samples": 52000,
    "val_samples": 11000,
    "questions_per_sample": 10,
    "duration": "1s",
    "quant": "4bit",
    "alignment": "EventGPT \u2192 Video-LLaVA (hidden states)"
  },
  "training": {
    "epochs": 300,
    "batch_size": 64,
    "learning_rate": 0.001,
    "early_stopping": 50,
    "optimizer": "AdamW",
    "scheduler": "CosineAnnealingLR",
    "loss": "MSE + 0.5 * CosLoss"
  },
  "timestamp": "20260207_203245",
  "best_val_loss": 1.216223868456754
}