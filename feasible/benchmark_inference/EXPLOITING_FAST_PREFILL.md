# Exploiting EventGPT's Fast Prefill to Accelerate Video-LLaVA

**Date:** 2026-01-24 20:45:00
**Key Insight:** EventGPT's 8.59x faster prefill (66ms vs 568ms) can be leveraged to skip Video-LLaVA's expensive prefill phase.

---

## 1. The Bottleneck Problem

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    VIDEO-LLAVA PREFILL BOTTLENECK                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Video-LLaVA (8 frames):                                                        │
│  ┌─────────────┐     ┌──────────────┐     ┌──────────────┐                        │
│  │ 8 RGB frames│ ──→ │ CLIP Encode  │ ──→ │ CONCATENATE   │                        │
│  │             │     │ (parallel)    │     │ 4608 tokens  │                        │
│  └─────────────┘     └──────┬───────┘     └──────┬───────┘                        │
│                             │                    │                                │
│                             ▼                    ▼                                │
│                    ┌─────────────────────────────────────┐                          │
│                    │    LLM PREFILL: 568ms ◄── BOTTLENECK                        │
│                    │    (4643 tokens × 32 layers)      │                          │
│                    └─────────────────────────────────────┘                          │
│                                      │                                           │
│                                      ▼ (KV cache ready)                       │
│                    ┌─────────────────────────────────────┐                          │
│                    │      DECODE: 35ms per token         │                          │
│                    └─────────────────────────────────────┘                          │
│                                                                                  │
│  Total: 568ms (prefill) + 35ms × N (decode)                                    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Video-LLaVA prefill = 76% of total time** for 5-token generation!

---

## 2. The Opportunity: EventGPT's Fast Prefill

```
EventGPT (1 frame):
  ┌─────────────┐     ┌──────────────┐     ┌──────────────┐
  │ 1 Event frame│ ──→ │ CLIP Encode  │ ──→ │ COMPRESS     │
  │             │     │ (5.93ms)     │     │ 636 tokens   │
  └─────────────┘     └──────┬───────┘     └──────┬───────┘
                             │                    │
                             ▼                    ▼
                    ┌─────────────────────────────────────┐
                    │   LLM PREFILL: 66ms ◄── 8.59x faster!
                    │   (636 tokens × 32 layers)        │
                    └─────────────────────────────────────┘
                                      │
                                      ▼ (KV cache ready)
                    ┌─────────────────────────────────────┐
                    │     DECODE: 18ms per token         │
                    └─────────────────────────────────────┘

Key: 66ms vs 568ms = 502ms advantage that could be transferred!
```

---

## 3. Approach 1: KV Cache Adaptation (Transfer Learning)

### 3.1 Core Idea

**Use EventGPT's KV cache as initialization for Video-LLaVA, skipping redundant prefill.**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                      KV CACHE TRANSFER PIPELINE                                   │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Step 1: EventGPT Prefill (FAST)                                               │
│  ┌──────────────────────────────────────┐                                      │
│  │ Input: 1 event frame                 │                                      │
│  │ Output: KV_cache_EGPT [32×636×128]   │                                      │
│  │ Time: 66ms                           │                                      │
│  └──────────────────────────────────────┘                                      │
│                  │                                                                  │
│                  ▼                                                                  │
│  Step 2: KV Cache Adapter                                                   │
│  ┌──────────────────────────────────────┐                                      │
│  │ Map: KV_cache_EGPT → KV_cache_VL     │                                      │
│  │ Strategy:                              │                                      │
│  │   - Positional expansion (636→4643)   │                                      │
│  │   - Feature projection (4096→4096)    │                                      │
│  │   - Temporal detail injection         │                                      │
│  │ Time: ~10-20ms                        │                                      │
│  └──────────────────────────────────────┘                                      │
│                  │                                                                  │
│                  ▼                                                                  │
│  Step 3: Video-LLaVA Decode (JUMPSTARTED)                                      │
│  ┌──────────────────────────────────────┐                                      │
│  │ Input: Adapted KV cache               │                                      │
│  │ Skip: Original 568ms prefill         │                                      │
│  │ Time: 35ms × N tokens                │                                      │
│  └──────────────────────────────────────┘                                      │
│                                                                                  │
│  Total: 66ms + 20ms = 86ms vs 568ms (6.6x faster prefill)                       │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 Adapter Architecture

```python
class KVCacheAdapter(nn.Module):
    """Transfer EventGPT KV cache to Video-LLaVA format."""

    def __init__(self):
        # Project EventGPT's compressed representations to Video-LLaVA's expanded format
        self.key_projection = nn.Linear(128, 128)  # Head dim stays same
        self.value_projection = nn.Linear(128, 128)

        # Learn temporal expansion (1 frame → 8 frames)
        self.temporal_expander = TemporalExpansionModule(
            in_tokens=636,
            out_tokens=577,  # 8 frames × 576 patches + 1
            num_heads=32
        )

        # Position embedding adjustment
        self.pos_embedding = nn.Embedding(4643, 4096)

    def forward(self, egpt_kv_cache, vl_attention_mask):
        """
        Args:
            egpt_kv_cache: Tuple of (K, V) from EventGPT
                K, V shape: [batch, num_heads, seq_egpt, head_dim]
                seq_egpt = 636
            vl_attention_mask: Attention mask for Video-LLaVA [batch, 4643]
        Returns:
            adapted_kv_cache: Tuple for Video-LLaVA
                K, V shape: [batch, num_heads, seq_vl, head_dim]
                seq_vl = 4643
        """
        K_egpt, V_egpt = egpt_kv_cache

        # 1. Expand tokens temporally (636 → 4643)
        K_expanded = self.temporal_expander(K_egpt)
        V_expanded = self.temporal_expander(V_egpt)

        # 2. Add positional information for expanded positions
        batch, num_heads, seq_vl, head_dim = K_expanded.shape
        positions = torch.arange(seq_vl, device=K_expanded.device)
        pos_emb = self.pos_embedding(positions).unsqueeze(0).unsqueeze(0)

        # 3. Projection for domain alignment
        K_adapted = self.key_projection(K_expanded) + pos_emb
        V_adapted = self.value_projection(V_expanded)

        return (K_adapted, V_adapted)
```

### 3.3 Temporal Expansion Module

```python
class TemporalExpansionModule(nn.Module):
    """Learn to expand EventGPT's compressed tokens into Video-LLaVA's frame-level tokens."""

    def __init__(self, in_tokens=636, out_tokens=4643, num_heads=32):
        super().__init__()

        # EventGPT structure: [577 vision] + [59 text] = 636
        # Video-LLaVA structure: [576×8 vision] + [35 text] = 4643

        self.vision_tokens_per_frame = 576
        self.num_vl_frames = 8

        # Learn how to distribute 577 compressed vision tokens into 4608 expanded tokens
        self.vision_expander = nn.Sequential(
            nn.Linear(4096, 8 * 576),  # Predict 8 frames worth of tokens
            nn.Unflatten(1, (8, 576)),
            nn.LayerNorm(576),
        )

        # Text tokens stay similar
        self.text_expander = nn.Linear(4096, 4096)

    def forward(self, hidden_states):
        """
        Args:
            hidden_states: [batch, 636, 4096]
        Returns:
            expanded: [batch, 4643, 4096]
        """
        # Split vision and text
        vision_hidden = hidden_states[:, :577, :]   # [batch, 577, 4096]
        text_hidden = hidden_states[:, 577:, :]    # [batch, 59, 4096]

        # Expand vision tokens
        vision_expanded = self.vision_expander(vision_hidden)  # [batch, 8, 576, 4096]
        vision_expanded = vision_expanded.reshape(hidden_states.shape[0], 4608, -1)

        # Expand text tokens
        text_expanded = self.text_expander(text_hidden)  # [batch, 59, 4096]

        # Concatenate
        expanded = torch.cat([vision_expanded, text_expanded], dim=1)  # [batch, 4667, ...]

        # Trim to exact size
        return expanded[:, :4643, :]
```

### 3.4 Training Procedure

```python
def train_kv_adapter():
    """
    Train the KV cache adapter using distillation from Video-LLaVA's prefill.
    """

    for batch in dataloader:
        images_8f, text = batch

        with torch.no_grad():
            # 1. Run EventGPT prefill (fast)
            egpt_kv = run_eventgpt_prefill(images_8f[:, :1], text)

            # 2. Run Video-LLaVA prefill (slow, but cached for training)
            vl_kv = run_videollava_prefill(images_8f, text)

        # 3. Adapter predicts VL KV from EGPT KV
        predicted_kv = kv_adapter(egpt_kv)

        # 4. Distillation loss: match VL's KV cache
        loss = kv_distillation_loss(predicted_kv, vl_kv)

        # 5. Auxiliary: verify that using predicted KV works
        verification_loss = verify_generation(images_8f, text, predicted_kv)

        total_loss = loss + 0.1 * verification_loss
        total_loss.backward()
        optimizer.step()
```

### 3.5 Expected Performance

```
Without KV Adapter:
  Video-LLaVA prefill: 568ms
  Video-LLaVA decode (5 tok): 35ms × 5 = 175ms
  Total: 743ms

With KV Adapter:
  EventGPT prefill: 66ms
  KV adaptation: 20ms
  Video-LLaVA decode (5 tok): 35ms × 5 = 175ms
  Total: 261ms

Speedup: 743ms / 261ms = 2.85x
```

---

## 4. Approach 2: Two-Stage Prefill (Hybrid)

### 4.1 Core Idea

**Use EventGPT for "coarse" prefill, Video-LLaVA for "refinement" only where needed.**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                      TWO-STAGE PREFILL PIPELINE                                   │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Stage 1: EventGPT Coarse Prefill (FAST)                                          │
│  ┌──────────────────────────────────────┐                                      │
│  │ • Process all 8 event frames          │                                      │
│  │ • Compressed to 650 tokens            │                                      │
│  │ • Generate "draft" hidden states     │                                      │
│  │ • Time: ~100ms                        │                                      │
│  └──────────────────────────────────────┘                                      │
│                  │                                                                  │
│                  ▼                                                                  │
│  Stage 2: Selective Refinement                                                  │
│  ┌──────────────────────────────────────┐                                      │
│  │ Question: Which regions need more detail?                                   │
│  │ • Static regions: Use EGPT as-is      │                                      │
│  │ • Motion regions: Refine with VL       │                                      │
│  │ Time: ~100-200ms (selective)           │                                      │
│  └──────────────────────────────────────┘                                      │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 Implementation

```python
class TwoStagePrefill:
    """Use EventGPT for fast coarse prefill, refine only where needed."""

    def __init__(self, eventgpt, videollava, motion_detector):
        self.eventgpt = eventgpt
        self.videollava = videollava
        self.motion_detector = motion_detector  # Fast motion detector

    def prefill(self, images, query):
        """
        Two-stage prefill strategy.
        """
        # Stage 1: Fast EventGPT prefill on all frames
        egpt_kv, egpt_hidden = self.eventgpt_prefill(images, query)

        # Stage 2: Identify regions needing refinement
        motion_map = self.motion_detector(images)

        if motion_map.max() < threshold:
            # Static scene: EGPT is sufficient
            return self.format_for_videollava(egpt_kv), "egpt_only"
        else:
            # Dynamic scene: Refine motion regions with VL
            motion_frames = [images[i] for i in motion_map.hotspots()]
            vl_refinement = self.videollava_refine(
                motion_frames,
                query,
                base_kv=egpt_kv  # Start from EGPT KV
            )
            return vl_refinement, "hybrid"
```

---

## 5. Approach 3: Parallel Vision Processing

### 5.1 Core Idea

**Process EventGPT and Video-LLaVA vision in parallel, merge during prefill.**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    PARALLEL VISION PROCESSING                                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Time ───────────────────────────────────────────────────────────────────→    │
│                                                                                  │
│  0ms    ┌───────────────────────────────────────────────────────┐             │
│         │             LOAD 8 VIDEO FRAMES                           │             │
│         └────────────────────────┬──────────────────────────────────┘             │
│                                  │                                                 │
│  50ms    ┌───────────────────────┴───────────────────────────────┐             │
│          │ PARALLEL ENCODING                                  │             │
│          │                                                    │             │
│          │  ┌─────────────┐         ┌─────────────────┐          │             │
│          │  │ EventGPT    │         │ Video-LLaVA     │          │             │
│          │  │ 1 frame     │         │ 8 frames        │          │             │
│          │  │             │         │                 │          │             │
│          │  │ CLIP: 6ms   │         │ CLIP: 29ms      │          │             │
│          │  │ Adaptor: 2ms│         │                 │          │             │
│          │  └──────┬──────┘         └─────┬───────────┘          │             │
│          │         │                      │                      │             │
│          │    EGPT: 577 tokens        VL: 4608 tokens             │             │
│          └─────────┼──────────────────────┼──────────────────────┘             │
│                    │                      │                              │
│  60ms               ▼                      ▼                              │
│          ┌────────────────────────────────────────────────────────────┐      │
│          │            MERGE & LLM PREFILL                              │      │
│          │                                                        │      │
│          │  Strategy: Process both token streams in parallel        │      │
│          │  • EventGPT tokens: attend to EGPT features               │      │
│          │  • VL "ghost" tokens: attend to VL features (async)      │      │
│          │                                                        │      │
│          └────────────────────────────────────────────────────────────┘      │
│                          │                                                   │
│  120ms                      ▼                                                   │
│          ┌────────────────────────────────────────────────────────────┐     │
│          │                    DECODE (joint)                          │     │
│          └────────────────────────────────────────────────────────────┘     │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 Async Prefill Architecture

```python
class AsyncParallelPrefill:
    """Run EventGPT and Video-LLaVA vision encoding in parallel."""

    def __init__(self, eventgpt, videollava):
        self.eventgpt = eventgpt
        self.videollava = videollava
        self.stream = torch.cuda.Stream()

    def parallel_prefill(self, images, query):
        """
        Parallel encoding with async KV cache merging.
        """
        # Start EventGPT (fast path)
        with torch.cuda.stream(self.stream):
            egpt_kv = self.eventgpt_prefill(images[:, :1], query)

        # Start Video-LLaVA (slow path) in background
        vl_future = self.videollava_prefill_async(images, query)

        # Wait for EventGPT to finish
        torch.cuda.synchronize()

        # Generate tokens from EGPT while VL processes
        draft_tokens = self.eventgpt.decode_first_n(
            egpt_kv,
            n=8  # Generate 8 tokens while VL works
        )

        # Wait for VL and merge KV caches
        vl_kv = vl_future.get()

        # Hybrid KV: EGPT for draft tokens, VL for verification
        merged_kv = self.merge_kv_caches(egpt_kv, draft_tokens, vl_kv)

        return merged_kv, draft_tokens
```

---

## 6. Performance Comparison

| Approach | Prefill Time | Decode Time | Total | Speedup |
|----------|-------------|-------------|-------|---------|
| **Baseline (VL only)** | 568 ms | 175 ms (5 tok) | 743 ms | 1.0x |
| **KV Adapter** | 86 ms | 175 ms | 261 ms | **2.85x** |
| **Two-Stage** | 150 ms (static) / 350 ms (dynamic) | 175 ms | 325-525 ms | 1.4-2.3x |
| **Parallel Vision** | 120 ms (async) | 175 ms | 295 ms | **2.52x** |
| **Optimal Hybrid** | 100 ms | 175 ms | 275 ms | **2.70x** |

---

## 7. Key Implementation Insights

### 7.1 Why KV Cache Adaptation Works

```
Intuition: EventGPT and Video-LLaVA process the same semantic content.

EventGPT (event camera):
  - Captures motion at high temporal resolution
  - Compresses spatio-temporal patterns
  - Produces 636 "dense" tokens

Video-LLaVA (RGB camera):
  - Captures appearance at high spatial resolution
  - Redundant per-frame encoding
  - Produces 4643 "sparse" tokens

Key insight: 636 EventGPT tokens contain MOST of the information
              needed for Video-LLaVA's 4643 tokens.

The adapter learns to "expand" dense → sparse representation.
```

### 7.2 Training Data Generation

```python
def generate_training_data():
    """
    Generate (EGPT_KV, VL_KV) pairs for adapter training.
    """
    dataset = []

    for video_frames, query in dataloader:
        # EventGPT prefill (fast)
        egpt_kv = run_eventgpt_prefill(video_frames[:, :1], query)

        # Video-LLaVA prefill (slow, but necessary for training)
        vl_kv = run_videollava_prefill(video_frames, query)

        # Token-wise alignment
        aligned_pairs = align_kv_caches(egpt_kv, vl_kv)

        dataset.extend(aligned_pairs)

    return dataset


def align_kv_caches(egpt_kv, vl_kv):
    """
    Align EventGPT and Video-LLaVA KV caches for distillation.
    """
    # Strategy: Use cross-attention to find correspondence
    # EGPT token i should correspond to VL tokens [i*8-4 : i*8+4]

    aligned = []
    for layer_idx in range(32):  # 32 layers
        egpt_K, egpt_V = egpt_kv[layer_idx]
        vl_K, vl_V = vl_kv[layer_idx]

        # Cross-attention finds alignment
        attention = torch.bmm(
            egpt_K.transpose(0, 1),  # [636, 128]
            vl_K  # [4643, 128]
        )  # [636, 4643]

        # Each EGPT token attends to VL tokens
        # Store pairs with highest attention
        top_k = 4  # Each EGPT token → 4 VL tokens
        top_idx = attention.topk(top_k, dim=-1).indices

        for egpt_i in range(636):
            for vl_j in top_idx[egpt_i]:
                aligned.append({
                    'layer': layer_idx,
                    'egpt_idx': egpt_i,
                    'vl_idx': vl_j,
                    'K_target': vl_K[:, :, vl_j],
                    'V_target': vl_V[:, :, vl_j],
                })

    return aligned
```

---

## 8. Summary: How to Exploit EventGPT's Fast Prefill

### Key Strategies

| Strategy | Mechanism | Expected Speedup | Complexity |
|----------|-----------|-----------------|------------|
| **KV Cache Adapter** | Transfer EGPT → VL KV | 2.85x | High (needs training) |
| **Two-Stage Prefill** | EGPT coarse, VL refine | 1.4-2.3x | Medium |
| **Parallel Vision** | Async encoding + merge | 2.52x | Medium |
| **Hybrid Optimal** | Combined approaches | **2.70x** | High |

### Critical Success Factors

1. **Adapter training is essential** - Cannot simply copy KV caches due to semantic differences
2. **Vision caching helps** - Pre-compute VL vision features, avoid 29ms encoding
3. **Token alignment matters** - Must learn which EGPT tokens map to which VL tokens
4. **Static scenes = maximum benefit** - When motion is low, EGPT alone may suffice

### Implementation Priority

1. **Phase 1**: Implement KV Cache Adapter (2-3 weeks)
2. **Phase 2**: Add vision caching (1 week)
3. **Phase 3**: Optimize with parallel processing (2 weeks)

**Target: 2.5-3x Video-LLaVA acceleration for static scenes, 1.8-2x for dynamic.**

---

*Analysis Date: 2026-01-24 20:45:00*
*Based on: 500ms Dataset Benchmark*
