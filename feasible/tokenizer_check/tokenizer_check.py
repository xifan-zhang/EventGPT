"""
è¯æ˜ EventGPT å’Œ VLM åœ¨æŠ€æœ¯å±‚é¢å¯ä»¥è¿›è¡Œ speculative decodingï¼Œ
å¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­â€œå®Œå…¨æ¨¡æ‹Ÿ inference.py é‡Œçš„è¡Œä¸ºâ€æ¥åŠ è½½ EventGPT çš„ tokenizerã€‚
"""

import os
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ modelã€dataset ç­‰åŒ…
# å½“å‰æ–‡ä»¶è·¯å¾„: <PROJECT_ROOT>/feasible/tokenizer_check/tokenizer_check.py
# å› æ­¤éœ€è¦å‘ä¸Šä¸‰çº§ç›®å½•ï¼Œæ‰èƒ½åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

import torch
import numpy as np
from transformers import AutoConfig, AutoTokenizer

from model.EventChatModel import EventChatModel
from dataset.constants import (
    EVENT_TOKEN_INDEX,
    DEFAULT_EVENT_TOKEN,
    DEFAULT_EV_START_TOKEN,
    DEFAULT_EV_END_TOKEN,
    EVENT_PLACEHOLDER,
    DEFAULT_EVENT_PATCH_TOKEN,
)

# âš ï¸ è¯·ç¡®ä¿è¿™é‡Œä¸æ¨ç†è„šæœ¬ä¸­ --model_path ä½¿ç”¨çš„è·¯å¾„ä¸€è‡´
# ä¾‹å¦‚ï¼špython inference.py --model_path "./checkpoints/EventGPT-7b" ...
EVENTGPT_MODEL_PATH = "./checkpoints/EventGPT-7b"


def load_eventgpt_tokenizer(model_path: str):
    """ä¸¥æ ¼æŒ‰ç…§ inference.py çš„æ–¹å¼åŠ è½½å¹¶æ‰©å±• EventGPT tokenizerã€‚"""
    config = AutoConfig.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

    # model = EventChatModel.from_pretrained(
    #     model_path,
    #     torch_dtype=torch.bfloat16,
    #     config=config,
    # )

    # # ä¸ inference.py ä¸­ç¬¬32â€“38è¡Œé€»è¾‘ä¿æŒä¸€è‡´
    # mm_use_im_start_end = getattr(model.config, "mm_use_im_start_end", False)
    # mm_use_im_patch_token = getattr(model.config, "mm_use_im_patch_token", True)

    # if mm_use_im_patch_token:
    #     tokenizer.add_tokens([DEFAULT_EVENT_PATCH_TOKEN], special_tokens=True)
    # if mm_use_im_start_end:
    #     tokenizer.add_tokens([DEFAULT_EV_START_TOKEN, DEFAULT_EV_END_TOKEN], special_tokens=True)

    # # ä¸ inference.py ç¬¬39è¡Œä¸€è‡´ï¼šæ ¹æ®æ–°è¯è¡¨å¤§å°è°ƒæ•´ embedding
    # model.resize_token_embeddings(len(tokenizer))

    return tokenizer

# è¯¦ç»†è¯Šæ–­è„šæœ¬
from transformers import AutoTokenizer
import json

def diagnose_tokenizer_difference(eventgpt_tokenizer, vlm_tokenizer):
    """
    è¯¦ç»†åˆ†æä¸¤ä¸ª tokenizer çš„å·®å¼‚
    """
    print("="*80)
    print("TOKENIZER COMPATIBILITY DIAGNOSIS")
    print("="*80)
    
    # 1. åŸºç¡€ä¿¡æ¯å¯¹æ¯”
    print("\nğŸ“Š Basic Information:")
    print(f"EventGPT vocab size: {len(eventgpt_tokenizer)}")
    print(f"VLM vocab size: {len(vlm_tokenizer)}")
    print(f"Difference: {len(vlm_tokenizer) - len(eventgpt_tokenizer)} tokens")
    
    # 2. ç‰¹æ®Š tokens å¯¹æ¯”
    print("\nğŸ”‘ Special Tokens:")
    special_tokens_info = {
        "bos_token": (eventgpt_tokenizer.bos_token, vlm_tokenizer.bos_token),
        "eos_token": (eventgpt_tokenizer.eos_token, vlm_tokenizer.eos_token),
        "unk_token": (eventgpt_tokenizer.unk_token, vlm_tokenizer.unk_token),
        "pad_token": (eventgpt_tokenizer.pad_token, vlm_tokenizer.pad_token),
    }
    
    for token_type, (event_token, vlm_token) in special_tokens_info.items():
        match = "âœ…" if event_token == vlm_token else "âŒ"
        print(f"{token_type:12} EventGPT: {event_token!r:10} VLM: {vlm_token!r:10} {match}")
    
    # 3. æ‰¾å‡º VLM é¢å¤–çš„ tokens
    print("\nğŸ” Analyzing Extra Tokens in VLM:")
    
    # è·å–è¯æ±‡è¡¨
    eventgpt_vocab = eventgpt_tokenizer.get_vocab()
    vlm_vocab = vlm_tokenizer.get_vocab()
    
    # æ‰¾å‡º VLM ç‹¬æœ‰çš„ tokens
    vlm_only_tokens = set(vlm_vocab.keys()) - set(eventgpt_vocab.keys())
    
    print(f"VLM has {len(vlm_only_tokens)} unique tokens:")
    for token in sorted(vlm_only_tokens):
        token_id = vlm_vocab[token]
        print(f"  - '{token}' (ID: {token_id})")
    
    # 4. æ£€æŸ¥ added_tokens
    if hasattr(vlm_tokenizer, 'added_tokens_encoder'):
        print("\nâ• VLM Added Tokens:")
        for token, token_id in vlm_tokenizer.added_tokens_encoder.items():
            print(f"  - '{token}' (ID: {token_id})")
    
    # 5. æµ‹è¯•ç¼–ç ä¸€è‡´æ€§
    print("\nğŸ§ª Encoding Consistency Tests:")
    test_cases = [
        "The object is moving.",
        "A person walks in the scene.",
        "Hello world!",
        "This is a test sentence with numbers: 123456.",
    ]
    
    all_match = True
    for i, text in enumerate(test_cases, 1):
        event_ids = eventgpt_tokenizer.encode(text, add_special_tokens=False)
        vlm_ids = vlm_tokenizer.encode(text, add_special_tokens=False)
        
        match = event_ids == vlm_ids
        all_match = all_match and match
        
        status = "âœ…" if match else "âŒ"
        print(f"Test {i} {status}: '{text[:40]}...'")
        
        if not match:
            print(f"  EventGPT IDs: {event_ids}")
            print(f"  VLM IDs: {vlm_ids}")
    
    # 6. æœ€ç»ˆç»“è®º
    print("\n" + "="*80)
    print("COMPATIBILITY ASSESSMENT")
    print("="*80)
    
    return {
        "vocab_size_match": len(eventgpt_tokenizer) == len(vlm_tokenizer),
        "encoding_match": all_match,
        "extra_tokens": vlm_only_tokens,
        "num_extra": len(vlm_only_tokens)
    }

if __name__ == "__main__":
    # 1. æŒ‰ç…§ inference.py çš„è¡Œä¸ºåŠ è½½ EventGPT çš„ tokenizer
    eventgpt_tokenizer = load_eventgpt_tokenizer(EVENTGPT_MODEL_PATH)

    # 2. åŠ è½½ VLM tokenizerï¼ˆä¿æŒä½ åŸæ¥çš„è®¾å®šï¼‰
    vlm_tokenizer = AutoTokenizer.from_pretrained("llava-hf/llava-1.5-13b-hf")
    # vlm_tokenizer = AutoTokenizer.from_pretrained("llava-hf/llava-1.5-7b-hf")

    # 3. æ‰“å°è¯è¡¨å¤§å°ï¼ŒåšåŸºæœ¬ sanity check
    print(f"EventGPT vocab size: {len(eventgpt_tokenizer)}") # EventGPT vocab size: 32000
    print(f"VLM vocab size: {len(vlm_tokenizer)}") # VLM vocab size: 32002
    
    # å·®ä¸¤ä¸ªï¼Œæ‰§è¡Œè¯Šæ–­
    result = diagnose_tokenizer_difference(eventgpt_tokenizer, vlm_tokenizer)

    # 4. æµ‹è¯•ç›¸åŒæ–‡æœ¬çš„ç¼–ç ç»“æœï¼ˆåªæ¯”è¾ƒè‡ªç„¶è¯­è¨€éƒ¨åˆ†çš„ç¼–ç æ˜¯å¦ä¸€è‡´ï¼‰
    test_texts = [
        "The object is moving rapidly from left to right.",
        "A person is holding something in the scene.",
        "The motion trajectory shows acceleration.",
    ]

    for text in test_texts:
        event_ids = eventgpt_tokenizer.encode(text)
        vlm_ids = vlm_tokenizer.encode(text)
        assert event_ids == vlm_ids, f"Tokenization mismatch for: {text}"

    print("âœ“ Tokenizer compatibility confirmed (with inference-style EventGPT tokenizer)!")