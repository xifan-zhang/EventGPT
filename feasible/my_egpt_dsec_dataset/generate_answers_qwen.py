#!/usr/bin/env python3
"""
Script to create EventGPT DSEC dataset and generate answers using Qwen VL models.

This script creates a complete HuggingFace-compatible dataset by:
1. Scanning event .npy files generated by build_my_egpt_dsec_dataset.py
2. For each event, trying top10 questions and selecting the one with highest model confidence
3. Filtering to keep only samples with confidence >= 90%
4. Generating descriptive answers using Qwen VL model with camera images only
5. Saving complete JSON dataset for HuggingFace import

Features:
- Supports multiple Qwen models (Qwen2-VL, Qwen3-VL)
- Creates dataset from scratch (no existing JSON required)
- Processes entire dataset or specific sequences
- Uses camera images for visual context (no event data analysis)
- Generates answers with Qwen model's native confidence scoring (probabilities)
- Filters to keep only high-confidence samples (>= 90% confidence)
- Shows progress with detailed progress bars, confidence metrics, and keeping rate ğŸ’
- Creates HuggingFace-compatible JSON format
- Clears previous output files automatically

Supported Models:
    - qwen3-8b:    Qwen/Qwen3-VL-8B-Instruct (Default)
    - qwen2.5-7b:  Qwen/Qwen2.5-VL-7B-Instruct
    - qwen2-7b:    Qwen/Qwen2-VL-7B-Instruct
    - qwen2-2b:    Qwen/Qwen2-VL-2B-Instruct
    - qwen2-72b:   Qwen/Qwen2-VL-72B-Instruct

Dataset Format:
    The output JSON follows the EventGPT format:
    {
      "id": "my_dsec_00000000",
      "event_data": "synthetic_000000.npy",
      "conversations": [
        {"from": "human", "value": "<event>\\nQuestion?"},
        {"from": "gpt", "value": "Answer... Confidence: 0.984"}
      ]
    }

Usage:
    # Use default model (Qwen3-VL-8B)
    python generate_answers_qwen.py --dataset_dir /mnt/hdd/data/my_egpt_dsec_dataset

    # Use Qwen2-VL-7B via alias
    python generate_answers_qwen.py --model_id qwen2-7b --dataset_dir /mnt/hdd/data/my_egpt_dsec_dataset

    # Use specific model ID
    python generate_answers_qwen.py --model_id Qwen/Qwen2-VL-2B-Instruct

    # Process specific sequence (e.g., thun_01_a)
    python generate_answers_qwen.py --sequence thun_01_a --dsec_root /mnt/hdd/data/DSEC/test

    # Limit samples for testing
    python generate_answers_qwen.py --max_samples 100 --dataset_dir /mnt/hdd/data/my_egpt_dsec_dataset

Requirements:
    pip install torch transformers qwen-vl-utils pillow numpy tqdm
"""

import os
import json
import argparse
import torch
import uuid
from PIL import Image
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForVision2Seq
from qwen_vl_utils import process_vision_info

# Try to import specific model classes
try:
    from transformers import Qwen3VLForConditionalGeneration
except ImportError:
    Qwen3VLForConditionalGeneration = None

try:
    from transformers import Qwen2VLForConditionalGeneration
except ImportError:
    Qwen2VLForConditionalGeneration = None

try:
    from transformers import Qwen2_5_VLForConditionalGeneration
except ImportError:
    Qwen2_5_VLForConditionalGeneration = None


# Configuration for supported models
MODEL_CONFIGS = {
    "qwen3-8b": {
        "id": "Qwen/Qwen3-VL-8B-Instruct",
        "type": "qwen3"
    },
    "qwen2-7b": {
        "id": "Qwen/Qwen2-VL-7B-Instruct",
        "type": "qwen2"
    },
    "qwen2-2b": {
        "id": "Qwen/Qwen2-VL-2B-Instruct",
        "type": "qwen2"
    },
    "qwen2-72b": {
        "id": "Qwen/Qwen2-VL-72B-Instruct",
        "type": "qwen2"
    },
    "qwen2.5-7b": {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "type": "qwen2.5"
    }
}


class QwenAnswerGenerator:
    """Qwen-based answer generator for EventGPT dataset using camera images only."""

    def __init__(self, model_id="Qwen/Qwen3-VL-8B-Instruct", device_map="auto"):
        """Initialize the answer generator with the specified model."""
        # Check if model_id is a short key in MODEL_CONFIGS
        if model_id in MODEL_CONFIGS:
            self.model_config = MODEL_CONFIGS[model_id]
            self.model_id = self.model_config["id"]
        else:
            # Assume it's a full HuggingFace model ID
            self.model_id = model_id
            self.model_config = self._infer_config(model_id)
            
        self.device_map = device_map
        self.model = None
        self.processor = None
        self.image_cache = {}  # Cache for loaded images

    def _infer_config(self, model_id):
        """Infer model configuration from model ID."""
        if "Qwen3-VL" in model_id:
            return {"id": model_id, "type": "qwen3"}
        elif "Qwen2.5-VL" in model_id:
            return {"id": model_id, "type": "qwen2.5"}
        elif "Qwen2-VL" in model_id:
            return {"id": model_id, "type": "qwen2"}
        else:
            # Default to auto/generic
            return {"id": model_id, "type": "auto"}

    def load_model(self):
        """Load the Qwen model and processor."""

        
        if self.model is None:
            print(f"ğŸ§  Loading model {self.model_id} (Type: {self.model_config['type']})...")
            try:
                # Select appropriate model class
                if self.model_config['type'] == 'qwen3':
                    if Qwen3VLForConditionalGeneration is None:
                        raise ImportError("Qwen3VLForConditionalGeneration not available in transformers")
                    model_class = Qwen3VLForConditionalGeneration
                elif self.model_config['type'] == 'qwen2.5':
                    if Qwen2_5_VLForConditionalGeneration is None:
                        # Fallback to AutoModel if specific class not found but might work
                        print("âš ï¸ Qwen2_5_VLForConditionalGeneration not found, trying AutoModelForVision2Seq")
                        model_class = AutoModelForVision2Seq
                    else:
                        model_class = Qwen2_5_VLForConditionalGeneration
                elif self.model_config['type'] == 'qwen2':
                    if Qwen2VLForConditionalGeneration is None:
                        # Fallback to AutoModel if specific class not found but might work
                        print("âš ï¸ Qwen2VLForConditionalGeneration not found, trying AutoModelForVision2Seq")
                        model_class = AutoModelForVision2Seq
                    else:
                        model_class = Qwen2VLForConditionalGeneration
                else:
                    model_class = AutoModelForVision2Seq

                self.model = model_class.from_pretrained(
                    self.model_id,
                    dtype="auto",
                    device_map=self.device_map
                )
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                print("âœ… Model loaded successfully!")
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Please ensure torch, transformers, and qwen-vl-utils are installed.")
                raise e


    def load_camera_image(self, event_rel_path, dsec_root):
        """
        Load the corresponding camera image for an event file, with caching.

        Returns:
            PIL Image: The camera image
        """
        # Parse sequence and frame info
        parts = event_rel_path.split('/')
        seq_name = parts[0]
        frame_name = parts[1].replace('.npy', '')  # e.g., "000000"

        # Create cache key from sequence and frame
        cache_key = f"{seq_name}/{frame_name}"

        # Check cache first
        if cache_key in self.image_cache:
            return self.image_cache[cache_key]

        # Load camera image
        image_path = os.path.join(dsec_root, seq_name, 'images', 'left', 'rectified', f"{frame_name}.png")
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            image = Image.new('RGB', (640, 480), color='gray')

        # Cache the image
        self.image_cache[cache_key] = image

        return image

    def generate_answer_with_confidence(self, question, camera_image):
        """Generate an answer for a question given a camera image and return with confidence."""
        if self.model is None:
            self.load_model()

        # Prepare inputs with camera image only
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Here is a camera image:"},
                    {"type": "image", "image": camera_image},
                    {"type": "text", "text": f"Question: {question}"},
                ],
            }
        ]

        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)

        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.model.device)

        # Generate with return_dict_in_generate and output_scores for confidence
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=128,
                return_dict_in_generate=True,
                output_scores=True,
                do_sample=False,  # Use greedy decoding for confidence
                temperature=0.0  # Set to 0 for deterministic output
            )

        generated_ids = outputs.sequences
        
        # Calculate transition scores (log probabilities)
        # This normalizes the logits to log probabilities
        transition_scores = self.model.compute_transition_scores(
            outputs.sequences, outputs.scores, normalize_logits=True
        )

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        # Calculate confidence as average log probability of generated tokens
        # transition_scores contains log probs for generated tokens only
        if len(transition_scores) > 0:
            # Get log probs for the first sequence in batch (we use batch_size=1)
            # transition_scores shape is (batch_size, sequence_length)
            log_probs = transition_scores[0]
            
            # Filter out padding tokens if any (though compute_transition_scores usually handles valid tokens)
            # For greedy decoding, we just take the mean of all generated token log probs
            avg_log_prob = torch.mean(log_probs).item()
            
            # Convert log probability to probability: exp(log_prob)
            confidence = torch.exp(torch.tensor(avg_log_prob)).item()
        else:
            confidence = 0.0

        return output_text, confidence

        return output_text, confidence

    def generate_answers_for_dataset(self, dataset, event_npy_dir, dsec_root, questions, sequence_filter=None):
        """Generate answers for a dataset with progress tracking, selecting best question per event."""
        if self.model is None:
            self.load_model()

        # Filter by sequence if specified
        if sequence_filter:
            print(f"ğŸ¯ Filtering for sequence: {sequence_filter}")
            original_count = len(dataset)
            dataset = [entry for entry in dataset if entry['event_data'].startswith(f"{sequence_filter}/")]
            print(f"âœ¨ Filtered from {original_count} to {len(dataset)} entries for sequence {sequence_filter}")
            if len(dataset) == 0:
                print(f"ğŸ˜” No entries found for sequence {sequence_filter}")
                return dataset

        print(f"ğŸš€ Generating answers for {len(dataset)} samples using {len(questions)} questions each...")

        current_sequence = None
        confidence_scores = []
        kept_samples = 0

        # Create tqdm with custom postfix for average confidence
        pbar = tqdm(dataset, desc="ğŸ¤– Processing samples", unit="sample")

        for entry in pbar:
            # Clear image cache when switching sequences to save memory
            entry_sequence = entry["event_data"].split('/')[0]
            if current_sequence != entry_sequence:
                if current_sequence is not None:
                    self.image_cache.clear()  # Clear cache when switching sequences
                current_sequence = entry_sequence

            # Load camera image corresponding to the event file
            event_rel_path = entry["event_data"]
            event_full_path = os.path.join(event_npy_dir, event_rel_path)

            if not os.path.exists(event_full_path):
                print(f"Warning: Event file not found {event_full_path}")
                continue

            # Load camera image
            camera_image = self.load_camera_image(event_rel_path, dsec_root)

            # Try all questions and find the one with highest confidence
            best_question = None
            best_answer = None
            best_confidence = -1.0

            for question in questions:
                answer, confidence = self.generate_answer_with_confidence(question, camera_image)

                if confidence > best_confidence:
                    best_confidence = confidence
                    best_question = question
                    best_answer = answer

            # Update the entry with the best question-answer pair
            if best_question and best_answer:
                entry["conversations"][0]["value"] = f"<event>\n{best_question}"
                entry["conversations"][1]["value"] = f"{best_answer} (Confidence: {best_confidence*100:.1f}%)"

                # Track confidence for average calculation
                confidence_scores.append(best_confidence)
                avg_confidence = sum(confidence_scores) / len(confidence_scores)

                # Check if this sample meets the 90% confidence threshold
                confidence_threshold = 0.5  # 50% confidence level
                if best_confidence >= confidence_threshold:
                    kept_samples += 1

                keeping_rate = kept_samples / len(confidence_scores) * 100

                # Update progress bar with current average confidence and keeping rate
                pbar.set_postfix({
                    'ğŸ“Š avg_conf': f'{avg_confidence*100:.1f}%',
                    'ğŸ¯ current_conf': f'{best_confidence*100:.1f}%',
                    'ğŸ’ keep_rate': f'{keeping_rate:.1f}%'
                })

        # Filter dataset to only keep samples with confidence >= 50%
        print(f"ğŸ¯ Filtering for confidence >= 50%: kept {kept_samples}/{len(dataset)} samples ({keeping_rate:.1f}%)")

        # We need to associate each entry with its confidence score
        # Create a list of tuples (entry, confidence) then filter and return entries
        entries_with_confidence = []
        for i, entry in enumerate(dataset):
            if i < len(confidence_scores) and confidence_scores[i] >= 0.5:
                entries_with_confidence.append(entry)

        return entries_with_confidence

def load_questions(questions_file):
    """Load questions from the top50 questions file."""
    questions = []
    with open(questions_file, 'r', encoding='utf-8') as f:
        for line in f:
            # Skip frequency count and extract question
            parts = line.strip().split('\t', 1)
            if len(parts) == 2:
                questions.append(parts[1])
    return questions

def scan_event_files(event_npy_dir, sequence_filter=None):
    """Scan event_npy directory and return list of event file paths."""
    event_files = []

    if not os.path.exists(event_npy_dir):
        print(f"Event directory not found: {event_npy_dir}")
        return event_files

    # Walk through all subdirectories (sequences)
    for root, dirs, files in os.walk(event_npy_dir):
        for file in files:
            if file.endswith('.npy'):
                rel_path = os.path.relpath(os.path.join(root, file), event_npy_dir)
                seq_name = os.path.dirname(rel_path)

                # Filter by sequence if specified
                if sequence_filter and seq_name != sequence_filter:
                    continue

                event_files.append(rel_path)

    return sorted(event_files)

def create_dataset_structure(event_files):
    """Create the initial dataset structure - one entry per event file."""
    dataset = []

    for event_file in event_files:
        dataset.append({
            "id": str(uuid.uuid4()),
            "event_data": event_file,
            "conversations": [
                {
                    "from": "human",
                    "value": ""  # Will be filled with best question
                },
                {
                    "from": "gpt",
                    "value": ""  # Will be filled by Qwen
                }
            ]
        })

    return dataset

def main():
    parser = argparse.ArgumentParser(description="Generate answers using Qwen2.5-VL and create HuggingFace dataset")
    parser.add_argument("--dataset_dir", type=str, default="/mnt/hdd/data/my_egpt_dsec_dataset",
                        help="Path to the dataset directory")
    parser.add_argument("--questions_file", type=str,
                        default="/home/ps/Documents/code/EventGPT/feasible/analysis_datasets/results_egpt_dsec_split/dsec_questions_top10.txt",
                        help="Path to the questions file")
    parser.add_argument("--model_id", type=str, default="Qwen/Qwen3-VL-8B-Instruct",
                        help=f"HuggingFace model ID or short alias. Available aliases: {', '.join(MODEL_CONFIGS.keys())}")
    parser.add_argument("--batch_size", type=int, default=1, help="Inference batch size")
    parser.add_argument("--sequence", type=str, default=None,
                        help="Process only this sequence (e.g., 'thun_01_a')")
    parser.add_argument("--dsec_root", type=str, default="/mnt/hdd/data/DSEC/test",
                        help="Root directory of DSEC test dataset")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Limit number of samples for testing")
    args = parser.parse_args()

    event_npy_dir = os.path.join(args.dataset_dir, "event_npy")
    
    # Determine output filename based on sequence
    if args.sequence:
        output_filename = f"EventGPT_Instruction_Subset_{args.sequence}.json"
    else:
        output_filename = "EventGPT_Instruction_Subset.json"
        
    output_json_path = os.path.join(args.dataset_dir, output_filename)

    # Clear output file from previous run if it exists
    if os.path.exists(output_json_path):
        print(f"ğŸ—‘ï¸  Removing previous output file: {output_json_path}")
        os.remove(output_json_path)

    # Load questions
    print(f"ğŸ“š Loading questions from {args.questions_file}")
    questions = load_questions(args.questions_file)
    print(f"âœ… Loaded {len(questions)} questions")

    # Scan event files
    print(f"ğŸ” Scanning event files in {event_npy_dir}")
    event_files = scan_event_files(event_npy_dir, args.sequence)
    print(f"ğŸ“ Found {len(event_files)} event files")

    if args.max_samples:
        # For testing, limit the number of samples
        total_samples = len(event_files) * len(questions)
        if total_samples > args.max_samples:
            # Calculate how many event files we need
            num_event_files = min(len(event_files), (args.max_samples + len(questions) - 1) // len(questions))
            event_files = event_files[:num_event_files]
            print(f"Limited to {len(event_files)} event files for testing")

    # Create initial dataset structure
    print("ğŸ—ï¸  Creating dataset structure...")
    dataset = create_dataset_structure(event_files)
    print(f"ğŸ“Š Created dataset with {len(dataset)} entries")

    # Initialize answer generator
    generator = QwenAnswerGenerator(model_id=args.model_id)

    # Generate answers
    dataset = generator.generate_answers_for_dataset(
        dataset, event_npy_dir, args.dsec_root, questions, args.sequence
    )

    # Save complete dataset
    print(f"ğŸ’¾ Saving complete dataset to {output_json_path}")
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)

    print("ğŸ‰ Done!")


if __name__ == "__main__":
    main()


